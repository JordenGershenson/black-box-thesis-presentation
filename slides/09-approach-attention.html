      <div class="slide bg-surface" id="slide-9">
        <div class="title-zone">
          <h1>The Approach: Attention as Information Routing</h1>
        </div>
        
        <div class="content-zone">
          <div class="row gap-lg fill">
            <!-- The Debate -->
            <div class="col fill gap">
              <p style="font-size: 14px; font-weight: 600; color: var(--primary);">The Academic Debate</p>
              
              <!-- Skeptics -->
              <div class="card-danger" style="padding: 14px; border-radius: 6px;">
                <p style="font-size: 12px; font-weight: 600; color: var(--danger); margin-bottom: 6px;">Skeptics</p>
                <p style="font-size: 11px; color: var(--muted-fg); font-style: italic;">
                  "Attention is not Explanation" — weights don't always correlate with gradient importance.
                </p>
                <p style="font-size: 10px; color: #718096; margin-top: 4px;">Jain & Wallace, 2019</p>
              </div>
              
              <!-- Proponents -->
              <div class="card-success" style="padding: 14px; border-radius: 6px;">
                <p style="font-size: 12px; font-weight: 600; color: #276749; margin-bottom: 6px;">Proponents</p>
                <p style="font-size: 11px; color: var(--muted-fg); font-style: italic;">
                  "Attention is not <u>not</u> Explanation" — it offers a plausible view of internal routing.
                </p>
                <p style="font-size: 10px; color: #718096; margin-top: 4px;">Wiegreffe & Pinter, 2019</p>
              </div>
            </div>
            
            <!-- Our Stance -->
            <div class="col fill gap">
              <p style="font-size: 14px; font-weight: 600; color: var(--primary);">Our Position</p>
              
              <div style="background: #e8f4f8; padding: 18px; border-radius: 8px; border: 2px solid var(--accent);">
                <p style="font-size: 13px; color: var(--primary); margin-bottom: 12px; line-height: 1.5;">
                  We use attention to map the <strong>information pathway</strong> — the model's "field of view" — while acknowledging it represents 
                  <em>where the model looks</em>, not strict causality.
                </p>
                
                <div style="background: white; padding: 12px; border-radius: 4px;">
                  <p style="font-size: 11px; color: var(--muted-fg);">
                    <strong style="color: var(--accent);">Key distinction:</strong><br>
                    • <strong>Faithful explanations</strong> — perfectly reflect causal mechanisms<br>
                    • <strong>Plausible explanations</strong> — provide useful, interpretable insights
                  </p>
                </div>
              </div>
              
              <div style="background: var(--purple-light); padding: 14px; border-radius: 6px;">
                <p style="font-size: 12px; color: #553c9a;">
                  <strong>Goal:</strong> Generate <em>plausible</em> explanations that domain experts can evaluate against meteorological knowledge.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <div class="footnote-zone">
          <p>Based on Abnar & Zuidema (2020) "Attention Rollout" concept</p>
        </div>
      </div>